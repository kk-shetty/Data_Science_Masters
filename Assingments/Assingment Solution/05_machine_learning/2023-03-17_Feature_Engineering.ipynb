{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q1. What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values.\n",
    "\n",
    "* **Missing Values** in a dataset refers to the absence of data or information for a specific variables or observations in that dataset.\n",
    "* Handling missing values are essentials for several reasons such a as:\n",
    "    1. **Data Integrity**: Missing values can lead to error in data analysis, modeling, and interpretation. If not addressed, they can result in biased or incorrect conclusions. \n",
    "    2. **Statistical Analysis**: many statistical analysis and machine learning algorithms cannot handle missing data. Imputing or filling in missing values enables the application of these methods and ensures the accuracy of statistical summaries and inferences. \n",
    "    3. **Visualization**: Missin g values can effect the accuracy of data visualizations.\n",
    "    4. **Model Performance**: in machine learning, missing values can adversly impact the performance of predictive models. Most machine learning algorithms require complete data.\n",
    "* Algorithms that are not affected by missing values:\n",
    "    1. **Decision Tree**: Decision tree algorithms, such as CART (Classification and Regression Trees) and Random forests can naturally handle missing values during the tree building.\n",
    "    2. **Random Forests**: Random Forests are an ensemble learning method that combines multiple decision trees. They inherit the ability to handle missing values from the individual decision trees.\n",
    "    3. **Gradient Boosting Macines**: Algorithms like Gradient Boosting and XGBoost can handle missing values by incorporating them into their optimization process.\n",
    "    4. **Neural Networks**: Neural networks, especially deep learning models can be designed to handle missing values through specialized architectures and techniques like embedding layers or masking.\n",
    "    5. **K-Nearest Neighbors (KNN)**: the KNN algorithms makes predictions based on the similarity of data points. It can naturally handle missing values by ignoring them when computing distance between the data points.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q2. List down techniques used to handle missing data. Give an example of each with python code.\n",
    "* Missing values can be handled by imputimng the missing values. Below are few methods which can impute the missing values\n",
    "    1. Mean value imputation\n",
    "    2. Median value imputation\n",
    "    3. Mode value imputation\n",
    "    4. Random value imputation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   age_mean\n",
       "5    NaN  29.699118\n",
       "17   NaN  29.699118\n",
       "19   NaN  29.699118\n",
       "26   NaN  29.699118\n",
       "28   NaN  29.699118\n",
       "..   ...        ...\n",
       "859  NaN  29.699118\n",
       "863  NaN  29.699118\n",
       "868  NaN  29.699118\n",
       "878  NaN  29.699118\n",
       "888  NaN  29.699118\n",
       "\n",
       "[177 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. mean value imputation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df_titanic = sns.load_dataset('titanic')\n",
    "\n",
    "# Check the total missing values for each variables\n",
    "df_titanic.isnull().sum()   # 177 missing values for age \n",
    "\n",
    "# calculate mean age\n",
    "mean_age = df_titanic['age'].mean()\n",
    "\n",
    "# Fill the missing values with mean age (Can also be done in place)\n",
    "df_titanic['age_mean'] = df_titanic['age'].fillna(mean_age)\n",
    "\n",
    "# Verify the data\n",
    "df_titanic[df_titanic['age'].isnull()][['age', 'age_mean']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_mean</th>\n",
       "      <th>age_median</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.699118</td>\n",
       "      <td>28.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age   age_mean  age_median\n",
       "5    NaN  29.699118        28.0\n",
       "17   NaN  29.699118        28.0\n",
       "19   NaN  29.699118        28.0\n",
       "26   NaN  29.699118        28.0\n",
       "28   NaN  29.699118        28.0\n",
       "..   ...        ...         ...\n",
       "859  NaN  29.699118        28.0\n",
       "863  NaN  29.699118        28.0\n",
       "868  NaN  29.699118        28.0\n",
       "878  NaN  29.699118        28.0\n",
       "888  NaN  29.699118        28.0\n",
       "\n",
       "[177 rows x 3 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. Median value imputation - when there are outliers in the dxat points\n",
    "\n",
    "## Calculate the median value\n",
    "median_age = df_titanic['age'].median()\n",
    "\n",
    "# Fill the missing values with median age (Can also be done in place)\n",
    "df_titanic['age_median'] = df_titanic['age'].fillna(median_age)\n",
    "\n",
    "# Verify the data\n",
    "df_titanic[df_titanic['age'].isnull()][['age', 'age_mean', 'age_median']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 3. Mode value imputation - for categorical data\n",
    "\n",
    "# Identify the missing categorical data that can be imputed\n",
    "df_titanic.isnull().sum()\n",
    "# Emark column has two missing values that can be imputed with mode value\n",
    "\n",
    "# Find the mode value\n",
    "mode_value = df_titanic[df_titanic['embarked'].notnull()]['embarked'].mode()[0]\n",
    "\n",
    "# Fill the missing value with mode value\n",
    "df_titanic['embarked'].fillna(mode_value, inplace = True)\n",
    "\n",
    "# Verify\n",
    "df_titanic['embarked'].isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>age_random</th>\n",
       "      <th>age_median</th>\n",
       "      <th>age_mean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>NaN</td>\n",
       "      <td>35.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>NaN</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>NaN</td>\n",
       "      <td>29.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>NaN</td>\n",
       "      <td>24.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>859</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>863</th>\n",
       "      <td>NaN</td>\n",
       "      <td>16.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>868</th>\n",
       "      <td>NaN</td>\n",
       "      <td>42.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>878</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>NaN</td>\n",
       "      <td>19.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>29.699118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     age  age_random  age_median   age_mean\n",
       "5    NaN        35.0        28.0  29.699118\n",
       "17   NaN        21.0        28.0  29.699118\n",
       "19   NaN        29.0        28.0  29.699118\n",
       "26   NaN        24.0        28.0  29.699118\n",
       "28   NaN        16.0        28.0  29.699118\n",
       "..   ...         ...         ...        ...\n",
       "859  NaN        16.0        28.0  29.699118\n",
       "863  NaN        16.0        28.0  29.699118\n",
       "868  NaN        42.0        28.0  29.699118\n",
       "878  NaN        19.0        28.0  29.699118\n",
       "888  NaN        19.0        28.0  29.699118\n",
       "\n",
       "[177 rows x 4 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 4. Random value imputation - select random value from the non missing data and fill the missing values\n",
    "\n",
    "# Get the non missing values\n",
    "non_na_ages = df_titanic.age.dropna()\n",
    "\n",
    "# Calculate the missing values count\n",
    "missing_count = df_titanic.age.isnull().sum()\n",
    "\n",
    "# Generate random values from non missing values\n",
    "random_values = np.random.choice(non_na_ages, size=missing_count)\n",
    "\n",
    "# Create a series from missing values with index of actual missing rows.\n",
    "random_value_series = pd.Series(random_values, index=df_titanic.index[df_titanic['age'].isnull()])\n",
    "\n",
    "# Fill the missing value with random generated series\n",
    "df_titanic['age_random'] = df_titanic['age'].fillna(random_value_series)\n",
    "\n",
    "# Verify the result\n",
    "df_titanic[df_titanic['age'].isnull()][['age', 'age_random', 'age_median', 'age_mean']]\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q3. Explain the imbalanced data. What will happen if imbalanced data is not handled?\n",
    "* **Imbalanced data** is a situation in dataset where the distribution of classes or labels is not rouighly equal, meaning that one class significantly outnumbers the others. This imbalance can occur in various types of datasets, such as binary classification problems, multi-class classification problems or even regression tasks with imbalanced target values. \n",
    "* *Example* : In a binary classification problem where you are trying to predict whether an email is spam or not, you might have a dataset with 95% of the emails being non-spam (class 0) and only 5% being spam (class 1). In this case, the data is imbalanced because one class (non-spam) dominates the other (spam).\n",
    "* If imablance is n ot handled properly, several issues can arise:\n",
    "    1. Bias in model training: Most machine learning algorithms are designed to optimize accuracy, which can lead to biased model when faced with imbalanced data.\n",
    "    2. Poor generalization: mmodels trained on imbalnced data may not generalize well on new unseen data.\n",
    "    3. Loss of information: Imbalanced dataset may contain valuable information about the minority class that is underrepresented\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q4. What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required.\n",
    "1. **Up-sampling** refers to manually adding data samples to the minority classes in order to create a more balanced dataset.\n",
    "2. **Down-sampling** referse to removing records from majority class there by creating a more balanced dataset.\n",
    "**Example**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    897\n",
       "1    102\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Create dample imbalanced data set \n",
    "sample_size = 1000\n",
    "class_0 = 0.897\n",
    "class_1 = 1 - class_0\n",
    "class_0_sample_size = int(sample_size * class_0)\n",
    "class_1_sample_size = int(sample_size * class_1)\n",
    "\n",
    "class_0 = pd.DataFrame({\n",
    "    'f1' : np.random.normal(loc = 0, scale = 2, size = class_0_sample_size),\n",
    "    'f2' : np.random.normal(loc = 0, scale = 2, size = class_0_sample_size),\n",
    "    'target' : [0] * class_0_sample_size\n",
    "})\n",
    "\n",
    "class_1 = pd.DataFrame({\n",
    "    'f1' : np.random.normal(loc = 0, scale = 2, size = class_1_sample_size),\n",
    "    'f2' : np.random.normal(loc = 0, scale = 2, size = class_1_sample_size),\n",
    "    'target' : [1] * class_1_sample_size\n",
    "})\n",
    "\n",
    "df = pd.concat([class_0, class_1])\n",
    "\n",
    "# verfiy if the data is imablanced\n",
    "df['target'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    897\n",
       "1    897\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. Up-sampling\n",
    "''' \n",
    "Up-sampling is used in the following cases:\n",
    "1. when the minority class is important\n",
    "2. When the dataset is small\n",
    "3. When there is no risk of overwhelming the majority class\n",
    "'''\n",
    "\n",
    "# Import resample module from sklearn\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Divide the dataframe into two parts - majority and minority, based on target\n",
    "majority = df[df['target'] == 0]\n",
    "minority = df[df['target'] == 1]\n",
    "\n",
    "# Upsample the minority dataset\n",
    "minority_upsampled = resample(minority,\n",
    "                              replace=True,    # values from the minority dataset can be picked multple times\n",
    "                              n_samples=len(majority),\n",
    "                              random_state=42)\n",
    "\n",
    "# Check the size of df after upsampling\n",
    "minority_upsampled.shape\n",
    "\n",
    "# Concatenate majority df and upsampled minority df by resetting the index\n",
    "upsampled_df = pd.concat([majority, minority_upsampled]).reset_index(drop=True)\n",
    "\n",
    "# Verify if the df is balanced\n",
    "upsampled_df.target.value_counts()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(102, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    102\n",
       "1    102\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 2. Down-sampling\n",
    "'''\n",
    "Down-sampling can be used in below scenarios:\n",
    "1. When the majority class is overwhelming\n",
    "2. When we have limited computational capacity\n",
    "3. When the majority class is less important\n",
    "'''\n",
    "\n",
    "majority_downsampled = resample(majority,\n",
    "                                replace = False,\n",
    "                                n_samples = len(minority),\n",
    "                                random_state = 42)\n",
    "\n",
    "# Check the size of df after upsampling\n",
    "print(majority_downsampled.shape)\n",
    "\n",
    "# Concatenate majority df and upsampled minority df by resetting the index\n",
    "downsampled_df = pd.concat([majority_downsampled, minority]).reset_index(drop=True)\n",
    "\n",
    "# Verify if the df is balanced\n",
    "downsampled_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q5. What is data Augmentation? Explain SMOTE.\n",
    "**Data augmentation**\n",
    "* It refers to artificially increase the size of the datasetr by applying various transformations to the original data.\n",
    "* These transformations create new, slightly modified versions of the existing data, which can be used to train machine learning models.\n",
    "* Data augmentation serves several purposes, including improving model generalization, reducing overfitting, and addressing issues related to limited training data.\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique)**\n",
    "* It is a data augmentation technique commonly used to address class imbalance in binary classification tasks.\n",
    "* It is particularly useful when the minority class is underrepresented in the dataset.\n",
    "* SMOTE works by generating synthetic examples for the minority class, thereby balancing the class distribution.\n",
    "* For each instance in the minority class, SMOTE randomly selects a similar instance (i.e., a neighbor) from the minority class. The similarity is determined using a distance metric such as Euclidean distance in feature space.\n",
    "* SMOTE then creates synthetic samples by interpolating between the selected instance and its chosen neighbor. It does this by choosing a random value between 0 and 1 and multiplying it by the difference between the feature vectors of the two instances. This random value determines the direction and extent of the interpolation.\n",
    "* The synthetic samples are added to the original dataset, effectively increasing the size of the minority class.\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q6. What are outliers in a dataset? Why is it essential to handle outliers?\n",
    "\n",
    "**Outliers**:\n",
    "* Outliers are data points in a dataset that significantly deviate from the majority of other data points. \n",
    "* They are observations that are unusually distant from the center or the typical values of a dataset. \n",
    "* Outliers can occur for various reasons, including data entry errors, measurement errors, natural variability, or the presence of rare events.\n",
    "\n",
    "**Detecting and handling outliers is important for several reasons:**\n",
    "* **Data Quality** : Outliers can be the result of data entry errors or measurement inaccuracies. By identifying and addressing outliers, we can improve the overall quality and accuracy of our dataset.\n",
    "* **Statistical Analysis**: Outliers can distort summary statistics, such as the mean and standard deviation, leading to inaccurate interpretations of the data. \n",
    "* **Model Performance**: Outliers can have a significant impact on the performance of machine learning and statistical models. Many models are sensitive to outliers, and their presence can lead to biased parameter estimates, reduced predictive accuracy, and increased model complexity.\n",
    "* **Risk Management**: In applications like finance, fraud detection, and quality control, outliers may represent important events or anomalies that require attention. Detecting and managing outliers can help mitigate risks and make informed decisions.\n",
    "___\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q7. You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?\n",
    "We can handle the missing values by imputing them with following techniques such as \n",
    "1. Mean value imputation.\n",
    "2. Median value imputation if there are any outliers in the existing dataset.\n",
    "3. Mode value imputation for categorical data.\n",
    "4. Random value imputation if the values are missing at random.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q8. You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?\n",
    "Determining whether the missing data is missing at random (MAR) or if there is a pattern to the missing data is an important step in handling missing data effectively. We can use some of the below strategy to investigate missing data pattern.\n",
    "1. **Visualization of Missing Data**: Visualizing missing data using techniques like missing data heatmaps or bar charts is a widely used and intuitive approach to initially explore the patterns of missingness in a dataset. It provides a quick visual summary of which variables have missing values and whether there are any apparent patterns.\n",
    "\n",
    "2. **Statistical Tests**: Statistical tests, such as Chi-square tests for independence or correlation tests, are frequently employed to determine if there is a significant association between the missingness of one variable and the values of other variables. These tests help quantify the relationship between missing data and other variables in the dataset.\n",
    "\n",
    "3. **Imputation and Analysis**: Imputing missing data and comparing the results before and after imputation is a common practice. If imputation significantly affects the analysis results, it may indicate non-random missingness. While not a direct method of identifying patterns, this approach can indirectly reveal whether missing data follows a systematic pattern.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q9. Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?\n",
    "- In this case we can perform one of the below methods to balance the data\n",
    "1. **Up-Sampling**: We can increase the size of minority class (patients with condition) by repeating the data sets randomly.\n",
    "2. **Down-Sampling**: We can decrease the size of majority class (patients without condition) to match the size of minority class by randomly deleteing the data records.\n",
    "2. **SMOTE (Synthetic Minority Over-Sampling Technique)**: SMOTE generates synthetic examples for the minority class, thereby balancing the class distribution.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q10. When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?\n",
    "\n",
    "- We can use **Down-Sampling** to balance the data set which deletes some records from majority class at random.\n",
    "- Below code explains how it can be done\n",
    "\n",
    "```python \n",
    "# First we need to import **resample** module from sklearn.utils to perform the resampling of data\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Divide th dataset into two dataframes - majority class and minority class\n",
    "df_majority = df[df['satisfaction'] == 1]\n",
    "df_minority = df[df['satisfaction'] == 0]\n",
    "\n",
    "# Down-Sample the majority class\n",
    "df_majority_down_sampled = resample(df_majority,\n",
    "                                    replace = False,    # This is to make sure that the data points are not repeated\n",
    "                                    n_samples = len(df_minority),    # The desired number of majority samples\n",
    "                                    random_state = 42\n",
    "                                    )\n",
    "\n",
    "# Now we combine the original minority and down sampled majority dataframes\n",
    "df = pd.concat([df_minority, df_majority_down_sampled], axis=0).reset_index(drop=True)\n",
    "```\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Q11. You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?\n",
    "\n",
    "- When we're dealing with an unbalanced dataset with a low percentage of occurrences of a rare event, it can lead to biased machine learning models that perform poorly on the minority class. \n",
    "- To address this issue, we can up-sample the minority at random or by generating synthetic examples of minority class using SMOTE\n",
    "\n",
    "1. **Up-Sampling Minority Class** : Here we increase the size of minority class by picking the data repeatedly at random.\n",
    "```python \n",
    "# First we need to import **resample** module from sklearn.utils to perform the resampling of data\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# Divide th dataset into two dataframes - majority class and minority class\n",
    "df_majority = df[df['occurrence'] == 0]\n",
    "df_minority = df[df['occurrence'] == 1]\n",
    "\n",
    "# Up-Sample the minority class\n",
    "df_minority_up_sampled = resample(df_minority,\n",
    "                                    replace = True,    # This is to make sure that the data points are repeated\n",
    "                                    n_samples = len(df_majority),    # The desired number of minority samples\n",
    "                                    random_state = 42\n",
    "                                    )\n",
    "\n",
    "# Now we combine the original majority and up sampled minority dataframes\n",
    "df = pd.concat([df_majority, df_minority_up_sampled], axis=0).reset_index(drop=True)\n",
    "```\n",
    "\n",
    "2. **Using SMOTE to generate synthetic examples of minority class**: SMOTE generates random examples of minority calss by genrating data beteween two nearest observations.\n",
    "```python\n",
    "# Import SMOTE from imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Create an inmstance of SMOTE\n",
    "oversample = SMOTE()\n",
    "\n",
    "# Transfer the data set by passing independent variable and dependent variable seperately\n",
    "X,y = oversample.fit_resample(df_final[['feature_1', 'feature_2']], df_final['occurrence'])\n",
    "\n",
    "# Comncatenate both dataframes\n",
    "oversampled_df = pd.concat([X, y], axis=1)\n",
    "```\n",
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
